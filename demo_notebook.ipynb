{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot Demo\n",
    "\n",
    "This notebook demonstrates how to use the RAG (Retrieval-Augmented Generation) chatbot for document processing and question answering. The chatbot can process Word documents, Excel files, and PDFs, and answer questions based on their content using OpenAI's language models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up our environment and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the parent directory to the path to import our modules\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our RAG chatbot modules\n",
    "from utils.document_processor import DocumentProcessor\n",
    "from utils.vector_store import VectorStore\n",
    "from rag_chatbot import RAGChatbot\n",
    "\n",
    "print(\"Modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the OpenAI API Key\n",
    "\n",
    "To use the RAG chatbot, you need an OpenAI API key. You can set it as an environment variable or provide it directly to the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set your OpenAI API key here\n",
    "# You can replace this with your actual API key or set it as an environment variable\n",
    "OPENAI_API_KEY = \"your-openai-api-key\"\n",
    "\n",
    "# Set the API key as an environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "\n",
    "print(\"API key set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Documents\n",
    "\n",
    "For demonstration purposes, let's create some sample text files that we can use to test the chatbot. In a real-world scenario, you would use your own PDF, Word, or Excel files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Create a sample directory\n",
    "sample_dir = \"./sample_docs\"\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "\n",
    "# Create a sample text file about artificial intelligence\n",
    "ai_content = \"\"\"\n",
    "# Artificial Intelligence Overview\n",
    "\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using the rules to reach approximate or definite conclusions), and self-correction.\n",
    "\n",
    "## Machine Learning\n",
    "\n",
    "Machine Learning is a subset of AI that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
    "\n",
    "The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "Deep Learning is a subset of machine learning that uses neural networks with many layers (hence \"deep\") to analyze various factors of data. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost.\n",
    "\n",
    "## Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) is a field of AI that gives computers the ability to understand text and spoken words in much the same way human beings can. NLP combines computational linguistics—rule-based modeling of human language—with statistical, machine learning, and deep learning models.\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(sample_dir, \"ai_overview.txt\"), \"w\") as f:\n",
    "    f.write(ai_content)\n",
    "\n",
    "# Create a sample text file about data science\n",
    "ds_content = \"\"\"\n",
    "# Data Science Fundamentals\n",
    "\n",
    "Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, and information science.\n",
    "\n",
    "## Data Analysis\n",
    "\n",
    "Data Analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains.\n",
    "\n",
    "## Big Data\n",
    "\n",
    "Big Data refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Big data challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source.\n",
    "\n",
    "## Data Visualization\n",
    "\n",
    "Data Visualization is the graphic representation of data. It involves producing images that communicate relationships among the represented data to viewers. Visualizing data is an important step in data analysis and is critical for understanding patterns, trends, and outliers in data.\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(sample_dir, \"data_science.txt\"), \"w\") as f:\n",
    "    f.write(ds_content)\n",
    "\n",
    "print(f\"Created sample documents in {sample_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing\n",
    "\n",
    "Now, let's explore how the document processor works. The `DocumentProcessor` class is responsible for extracting text from different document types and splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the document processor\n",
    "processor = DocumentProcessor(chunk_size=500, chunk_overlap=100)\n",
    "print(f\"Document processor initialized with chunk_size=500, chunk_overlap=100\")\n",
    "\n",
    "# Process a sample document\n",
    "ai_file_path = os.path.join(sample_dir, \"ai_overview.txt\")\n",
    "chunks = processor.process_file(ai_file_path)\n",
    "\n",
    "print(f\"Processed {ai_file_path}\")\n",
    "print(f\"Extracted {len(chunks)} chunks\")\n",
    "\n",
    "# Display the first chunk\n",
    "print(\"\\nFirst chunk:\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database\n",
    "\n",
    "Next, let's see how the vector database works. The `VectorStore` class uses Chroma to store and retrieve document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the vector store\n",
    "vector_store = VectorStore(persist_directory=\"./demo_chroma_db\")\n",
    "print(\"Vector store initialized\")\n",
    "\n",
    "# Add the chunks to the vector store\n",
    "metadatas = [{\"source\": \"ai_overview.txt\"} for _ in chunks]\n",
    "ids = vector_store.add_texts(chunks, metadatas)\n",
    "print(f\"Added {len(ids)} chunks to vector store\")\n",
    "\n",
    "# Process and add another document\n",
    "ds_file_path = os.path.join(sample_dir, \"data_science.txt\")\n",
    "ds_chunks = processor.process_file(ds_file_path)\n",
    "ds_metadatas = [{\"source\": \"data_science.txt\"} for _ in ds_chunks]\n",
    "ds_ids = vector_store.add_texts(ds_chunks, ds_metadatas)\n",
    "print(f\"Added {len(ds_ids)} chunks from data science document\")\n",
    "\n",
    "# Perform a similarity search\n",
    "query = \"What is machine learning?\"\n",
    "results = vector_store.similarity_search(query, k=2)\n",
    "\n",
    "print(f\"\\nSimilarity search results for query: '{query}'\")\n",
    "for i, doc in enumerate(results):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Source: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content[:150]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbot\n",
    "\n",
    "Now, let's use the RAG chatbot to answer questions based on our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize the RAG chatbot\n",
    "chatbot = RAGChatbot(persist_directory=\"./demo_rag_db\")\n",
    "print(\"RAG chatbot initialized\")\n",
    "\n",
    "# Load the sample documents\n",
    "file_paths = [\n",
    "    os.path.join(sample_dir, \"ai_overview.txt\"),\n",
    "    os.path.join(sample_dir, \"data_science.txt\")\n",
    "]\n",
    "num_chunks = chatbot.load_documents(file_paths=file_paths)\n",
    "print(f\"Loaded {num_chunks} chunks from {len(file_paths)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking Questions\n",
    "\n",
    "Let's ask some questions to the chatbot and see how it responds based on the documents we've loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define some questions to ask\n",
    "questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"Explain machine learning and how it relates to AI.\",\n",
    "    \"What is data science and how does it differ from AI?\",\n",
    "    \"What are the main components of data analysis?\",\n",
    "    \"How are deep learning and natural language processing related?\"\n",
    "]\n",
    "\n",
    "# Ask each question and display the answer\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"Question {i+1}: {question}\")\n",
    "    answer = chatbot.ask(question)\n",
    "    print(f\"Answer: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Chatbot with Your Own Documents\n",
    "\n",
    "To use the chatbot with your own documents, you would follow these steps:\n",
    "\n",
    "1. Initialize the chatbot with your OpenAI API key\n",
    "2. Load your documents (PDF, Word, Excel)\n",
    "3. Ask questions about the content of your documents\n",
    "\n",
    "Here's an example of how you would do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example code for using your own documents\n",
    "'''\n",
    "# Initialize the chatbot\n",
    "my_chatbot = RAGChatbot(openai_api_key=\"your-api-key\")\n",
    "\n",
    "# Load your documents\n",
    "my_documents = [\n",
    "    \"path/to/your/document.pdf\",\n",
    "    \"path/to/your/document.docx\",\n",
    "    \"path/to/your/spreadsheet.xlsx\"\n",
    "]\n",
    "my_chatbot.load_documents(file_paths=my_documents)\n",
    "\n",
    "# Or load all documents in a directory\n",
    "# my_chatbot.load_documents(directory_path=\"path/to/your/documents\")\n",
    "\n",
    "# Ask questions\n",
    "question = \"What information is in these documents?\"\n",
    "answer = my_chatbot.ask(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "'''\n",
    "\n",
    "print(\"Example code for using your own documents is shown above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "Finally, let's clean up by clearing the documents from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Clear documents from the chatbot\n",
    "chatbot.clear_documents()\n",
    "print(\"Cleared documents from the chatbot\")\n",
    "\n",
    "# Clear the vector store\n",
    "vector_store.clear()\n",
    "print(\"Cleared the vector store\")\n",
    "\n",
    "print(\"\\nDemo completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the RAG chatbot to process documents and answer questions based on their content. The chatbot uses:\n",
    "\n",
    "- Document processing to extract text from different file types\n",
    "- Vector database (Chroma) to store and retrieve document embeddings\n",
    "- OpenAI API to generate answers based on retrieved context\n",
    "\n",
    "You can use this chatbot with your own documents by following the steps outlined above. For a more user-friendly interface, you can also use the Streamlit app provided in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
