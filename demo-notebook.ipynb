{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chatbot Demo\n",
    "\n",
    "This notebook demonstrates how to use the RAG (Retrieval-Augmented Generation) chatbot for document processing and question answering. The chatbot can process Word documents, Excel files, and PDFs, and answer questions based on their content using OpenAI's language models.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up our environment and import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Make sure we're in the root directory of the repository\n",
    "# This helps ensure imports work correctly\n",
    "if os.path.basename(os.getcwd()) == 'test':\n",
    "    os.chdir('..')\n",
    "elif os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "\n",
    "# Import our RAG chatbot modules\n",
    "try:\n",
    "    from utils.document_processor import DocumentProcessor\n",
    "    from utils.vector_store import VectorStore\n",
    "    from rag_chatbot import RAGChatbot\n",
    "    print(\"Modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing modules: {e}\")\n",
    "    print(\"\\nMake sure you have installed all required dependencies.\")\n",
    "    print(\"You can install them using: conda env create -f environment.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the OpenAI API Key\n",
    "\n",
    "To use the RAG chatbot, you need an OpenAI API key. You can set it as an environment variable or provide it directly to the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3e57758178462ba233ecefd1082bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password(description='OpenAI API Key:', layout=Layout(width='500px'), placeholder='Enter your OpenAI API key')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c02b652f8f4097ae1364032c1de093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Apply API Key', layout=Layout(width='150px'), style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c458a8530384206bf52076f763eb81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "from ipywidgets import widgets\n",
    "\n",
    "# Create a simple password input widget\n",
    "api_key_input = widgets.Password(\n",
    "    description='OpenAI API Key:',\n",
    "    placeholder='Enter your key',\n",
    "    layout=widgets.Layout(width='400px')\n",
    ")\n",
    "\n",
    "# Function to set the API key when the user presses Enter\n",
    "def on_key_entered(sender):\n",
    "    key = sender.value.strip()\n",
    "    if key:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = key\n",
    "        print(\"✅ API key set\")\n",
    "    \n",
    "# Register the callback\n",
    "api_key_input.observe(lambda change: on_key_entered(api_key_input) \n",
    "                      if change.name == 'value' and change.new.endswith('\\n') \n",
    "                      else None, names='value')\n",
    "\n",
    "# Display status\n",
    "if os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    print(\"API key already set in environment\")\n",
    "else:\n",
    "    print(\"Enter your OpenAI API key and press Enter\")\n",
    "\n",
    "# Display the widget\n",
    "display(api_key_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Test Data\n",
    "\n",
    "The repository includes test data in the `test/test_data` directory. Let's check if these files are available for us to use in our demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to test data directory\n",
    "test_data_dir = \"test/test_data\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(test_data_dir):\n",
    "    print(f\"Test data directory found at {test_data_dir}\")\n",
    "    \n",
    "    # List the available files\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(test_data_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(('.pdf', '.docx', '.xlsx', '.xls')):\n",
    "                files.append(os.path.join(root, filename))\n",
    "    \n",
    "    if files:\n",
    "        print(\"\\nAvailable test files:\")\n",
    "        for file in files:\n",
    "            print(f\"- {file}\")\n",
    "    else:\n",
    "        print(\"\\nNo suitable test files found. We'll create sample files for the demo.\")\n",
    "else:\n",
    "    print(f\"Test data directory not found at {test_data_dir}\")\n",
    "    print(\"We'll create sample files for the demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Sample Documents (if needed)\n",
    "\n",
    "If we don't have test files available, let's create some sample text files that we can use to test the chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample files only if we don't have test files already\n",
    "if not os.path.exists(test_data_dir) or not files:\n",
    "    # Create a sample directory\n",
    "    sample_dir = \"./sample_docs\"\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    # Create a sample text file about artificial intelligence\n",
    "    ai_content = \"\"\"\n",
    "    # Artificial Intelligence Overview\n",
    "    \n",
    "    Artificial Intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using the rules to reach approximate or definite conclusions), and self-correction.\n",
    "    \n",
    "    ## Machine Learning\n",
    "    \n",
    "    Machine Learning is a subset of AI that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\n",
    "    \n",
    "    The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\n",
    "    \n",
    "    ## Deep Learning\n",
    "    \n",
    "    Deep Learning is a subset of machine learning that uses neural networks with many layers (hence \"deep\") to analyze various factors of data. Deep learning is a key technology behind driverless cars, enabling them to recognize a stop sign, or to distinguish a pedestrian from a lamppost.\n",
    "    \n",
    "    ## Natural Language Processing\n",
    "    \n",
    "    Natural Language Processing (NLP) is a field of AI that gives computers the ability to understand text and spoken words in much the same way human beings can. NLP combines computational linguistics—rule-based modeling of human language—with statistical, machine learning, and deep learning models.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(sample_dir, \"ai_overview.txt\"), \"w\") as f:\n",
    "        f.write(ai_content)\n",
    "    \n",
    "    # Create a sample text file about data science\n",
    "    ds_content = \"\"\"\n",
    "    # Data Science Fundamentals\n",
    "    \n",
    "    Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from structured and unstructured data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, and information science.\n",
    "    \n",
    "    ## Data Analysis\n",
    "    \n",
    "    Data Analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains.\n",
    "    \n",
    "    ## Big Data\n",
    "    \n",
    "    Big Data refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Big data challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source.\n",
    "    \n",
    "    ## Data Visualization\n",
    "    \n",
    "    Data Visualization is the graphic representation of data. It involves producing images that communicate relationships among the represented data to viewers. Visualizing data is an important step in data analysis and is critical for understanding patterns, trends, and outliers in data.\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(os.path.join(sample_dir, \"data_science.txt\"), \"w\") as f:\n",
    "        f.write(ds_content)\n",
    "    \n",
    "    print(f\"Created sample documents in {sample_dir}\")\n",
    "    \n",
    "    # Update our files list to use these samples\n",
    "    files = [\n",
    "        os.path.join(sample_dir, \"ai_overview.txt\"),\n",
    "        os.path.join(sample_dir, \"data_science.txt\")\n",
    "    ]\n",
    "else:\n",
    "    print(\"Using existing test files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing\n",
    "\n",
    "Now, let's explore how the document processor works. The `DocumentProcessor` class is responsible for extracting text from different document types and splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the document processor\n",
    "processor = DocumentProcessor(chunk_size=500, chunk_overlap=100)\n",
    "print(f\"Document processor initialized with chunk_size=500, chunk_overlap=100\")\n",
    "\n",
    "# Select a document to process\n",
    "if files:\n",
    "    test_file = files[0]  # Choose the first available file\n",
    "    print(f\"Processing {test_file}...\")\n",
    "    \n",
    "    try:\n",
    "        chunks = processor.process_file(test_file)\n",
    "        print(f\"Successfully processed {test_file}\")\n",
    "        print(f\"Extracted {len(chunks)} chunks\")\n",
    "        \n",
    "        # Display the first chunk\n",
    "        if chunks:\n",
    "            print(\"\\nFirst chunk:\")\n",
    "            preview_length = min(len(chunks[0]), 500)\n",
    "            print(chunks[0][:preview_length] + (\"...\" if preview_length < len(chunks[0]) else \"\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {str(e)}\")\n",
    "else:\n",
    "    print(\"No files available to process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database\n",
    "\n",
    "Next, let's see how the vector database works. The `VectorStore` class uses Chroma to store and retrieve document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if OpenAI API key is set\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key and OPENAI_API_KEY != \"your-openai-api-key\":\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    api_key = OPENAI_API_KEY\n",
    "\n",
    "if not api_key or api_key == \"your-openai-api-key\":\n",
    "    print(\"WARNING: OpenAI API key is not set. Vector store operations will be skipped.\")\n",
    "    print(\"Please set your API key in the cell above and run it again.\")\n",
    "else:\n",
    "    try:\n",
    "        # Initialize the vector store\n",
    "        vector_store = VectorStore(persist_directory=\"./demo_chroma_db\")\n",
    "        print(\"Vector store initialized successfully\")\n",
    "        \n",
    "        # Add the chunks to the vector store if we have any\n",
    "        if 'chunks' in locals() and chunks:\n",
    "            metadatas = [{\"source\": os.path.basename(test_file)} for _ in chunks]\n",
    "            ids = vector_store.add_texts(chunks, metadatas)\n",
    "            print(f\"Added {len(ids)} chunks to vector store\")\n",
    "            \n",
    "            # Try processing a second file if available\n",
    "            if len(files) > 1:\n",
    "                second_file = files[1]\n",
    "                print(f\"\\nProcessing second file: {second_file}\")\n",
    "                try:\n",
    "                    second_chunks = processor.process_file(second_file)\n",
    "                    second_metadatas = [{\"source\": os.path.basename(second_file)} for _ in second_chunks]\n",
    "                    second_ids = vector_store.add_texts(second_chunks, second_metadatas)\n",
    "                    print(f\"Added {len(second_ids)} chunks from second file\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing second file: {str(e)}\")\n",
    "            \n",
    "            # Perform a similarity search\n",
    "            print(\"\\nPerforming similarity search...\")\n",
    "            query = \"What is artificial intelligence?\"\n",
    "            results = vector_store.similarity_search(query, k=2)\n",
    "            \n",
    "            print(f\"Similarity search results for query: '{query}'\")\n",
    "            for i, doc in enumerate(results):\n",
    "                print(f\"Result {i+1}:\")\n",
    "                print(f\"Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "                preview_length = min(len(doc.page_content), 150)\n",
    "                print(f\"Content: {doc.page_content[:preview_length]}...\")\n",
    "                print()\n",
    "        else:\n",
    "            print(\"No chunks available to add to vector store\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with vector store operations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Chatbot\n",
    "\n",
    "Now, let's use the RAG chatbot to answer questions based on our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if OpenAI API key is set\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not api_key and OPENAI_API_KEY != \"your-openai-api-key\":\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "    api_key = OPENAI_API_KEY\n",
    "\n",
    "if not api_key or api_key == \"your-openai-api-key\":\n",
    "    print(\"WARNING: OpenAI API key is not set. RAG chatbot operations will be skipped.\")\n",
    "    print(\"Please set your API key in the cell above and run it again.\")\n",
    "else:\n",
    "    try:\n",
    "        # Initialize the RAG chatbot\n",
    "        chatbot = RAGChatbot(persist_directory=\"./demo_rag_db\")\n",
    "        print(\"RAG chatbot initialized successfully\")\n",
    "        \n",
    "        # Load the documents\n",
    "        if files:\n",
    "            print(f\"Loading {len(files)} documents...\")\n",
    "            num_chunks = chatbot.load_documents(file_paths=files)\n",
    "            print(f\"Loaded {num_chunks} chunks from {len(files)} documents\")\n",
    "        else:\n",
    "            print(\"No files available to load\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with RAG chatbot operations: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking Questions\n",
    "\n",
    "Let's ask some questions to the chatbot and see how it responds based on the documents we've loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the chatbot is initialized\n",
    "if 'chatbot' in locals() and api_key and api_key != \"your-openai-api-key\":\n",
    "    # Define some questions to ask\n",
    "    questions = [\n",
    "        \"What is artificial intelligence?\",\n",
    "        \"Explain machine learning and how it relates to AI.\",\n",
    "        \"What is data science and how does it differ from AI?\",\n",
    "        \"What are the main components of data analysis?\",\n",
    "        \"How are deep learning and natural language processing related?\"\n",
    "    ]\n",
    "    \n",
    "    # Ask each question and display the answer\n",
    "    for i, question in enumerate(questions):\n",
    "        print(f\"Question {i+1}: {question}\")\n",
    "        answer = chatbot.ask(question)\n",
    "        print(f\"Answer: {answer}\\n\")\n",
    "else:\n",
    "    print(\"Chatbot is not available. Please make sure you have set your OpenAI API key and initialized the chatbot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Chatbot with Your Own Documents\n",
    "\n",
    "To use the chatbot with your own documents, you would follow these steps:\n",
    "\n",
    "1. Initialize the chatbot with your OpenAI API key\n",
    "2. Load your documents (PDF, Word, Excel)\n",
    "3. Ask questions about the content of your documents\n",
    "\n",
    "Here's an example of how you would do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code for using your own documents\n",
    "'''\n",
    "# Initialize the chatbot\n",
    "my_chatbot = RAGChatbot(openai_api_key=\"your-api-key\")\n",
    "\n",
    "# Load your documents\n",
    "my_documents = [\n",
    "    \"path/to/your/document.pdf\",\n",
    "    \"path/to/your/document.docx\",\n",
    "    \"path/to/your/spreadsheet.xlsx\"\n",
    "]\n",
    "my_chatbot.load_documents(file_paths=my_documents)\n",
    "\n",
    "# Or load all documents in a directory\n",
    "# my_chatbot.load_documents(directory_path=\"path/to/your/documents\")\n",
    "\n",
    "# Ask questions\n",
    "question = \"What information is in these documents?\"\n",
    "answer = my_chatbot.ask(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "'''\n",
    "\n",
    "print(\"Example code for using your own documents is shown above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "Finally, let's clean up by clearing the documents from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources if they were created\n",
    "if 'chatbot' in locals() and api_key and api_key != \"your-openai-api-key\":\n",
    "    # Clear documents from the chatbot\n",
    "    chatbot.clear_documents()\n",
    "    print(\"Cleared documents from the chatbot\")\n",
    "\n",
    "if 'vector_store' in locals() and api_key and api_key != \"your-openai-api-key\":\n",
    "    # Clear the vector store\n",
    "    vector_store.clear()\n",
    "    print(\"Cleared the vector store\")\n",
    "\n",
    "print(\"\\nDemo completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the RAG chatbot to process documents and answer questions based on their content. The chatbot uses:\n",
    "\n",
    "- Document processing to extract text from different file types\n",
    "- Vector database (Chroma) to store and retrieve document embeddings\n",
    "- OpenAI API to generate answers based on retrieved context\n",
    "\n",
    "You can use this chatbot with your own documents by following the steps outlined above. For a more user-friendly interface, you can also use the Streamlit app provided in the repository by running:\n",
    "\n",
    "```bash\n",
    "python run_app.py\n",
    "```\n",
    "\n",
    "or directly with Streamlit:\n",
    "\n",
    "```bash\n",
    "streamlit run app.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
